{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b720b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #0\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3650 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Running Trial #1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.5047 - acc: 0.8000\n",
      "Accuracy: 80.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Running Trial #2\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.3617 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Running Trial #3\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.3786 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Running Trial #4\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4147 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Running Trial #5\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3912 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Running Trial #6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3473 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n",
      "Processing fold #1\n",
      "Validation Set: Subjects 4:8\n",
      "Processing fold #2\n",
      "Validation Set: Subjects 8:12\n",
      "Processing fold #3\n",
      "Validation Set: Subjects 12:16\n",
      "Processing fold #4\n",
      "Validation Set: Subjects 16:21\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #7\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3159 - acc: 0.9000\n",
      "Accuracy: 90.00\n",
      "Processing fold #0\n",
      "Validation Set: Subjects 0:4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 582\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;66;03m#kFold(model, k_train_x, k_train_y, kfold_subjects, k_fold_IDs)\u001b[39;00m\n\u001b[0;32m    581\u001b[0m dense_accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m( (accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m--> 582\u001b[0m dense_kfold_accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m(kFold(model, k_train_x, k_train_y, kfold_subjects, k_fold_IDs)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# instantiate the model (using the default parameters)\u001b[39;00m\n\u001b[0;32m    585\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn [15], line 91\u001b[0m, in \u001b[0;36mkFold\u001b[1;34m(model, train_x, train_y, subjects, IDs)\u001b[0m\n\u001b[0;32m     88\u001b[0m partial_train_data \u001b[38;5;241m=\u001b[39m train_x[train_indicies]\n\u001b[0;32m     89\u001b[0m partial_train_targets \u001b[38;5;241m=\u001b[39m train_y[train_indicies]\n\u001b[1;32m---> 91\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_train_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m loss, val_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_data, val_targets, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)     \n\u001b[0;32m     94\u001b[0m all_scores\u001b[38;5;241m.\u001b[39mappend(val_acc)\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\Users\\C23PRI~1.JOH\\AppData\\Local\\R-MINI~1\\envs\\cap2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot\n",
    "import statistics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.pyplot import figure\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# formating\n",
    "plot_size = (25,10) #set plot size (entire plot)\n",
    "\n",
    "# get indicies for time data\n",
    "def get_indices(time_data, start_time, end_time):\n",
    "    \"\"\"Return indecies of time_data that are >= start_time and < end_time\n",
    "    \n",
    "    time_data  - np array of type float, time_data indices will correlate to\n",
    "    start_time - float, a timestamp for the start\n",
    "    end_time   - float, a timestamp for the end\n",
    "    \n",
    "    return     - a tuple of indices\n",
    "    \"\"\"\n",
    "    return tuple(np.where(\n",
    "        np.logical_and(\n",
    "            np.greater_equal(time_data, start_time), np.less(time_data, end_time)\n",
    "        ))[0])\n",
    "\n",
    "def get_user_slice(target, subjects):\n",
    "    '''return slice for user - the indicies\n",
    "       Target - array you are slicing \n",
    "       Slices are done on target dataset, user must ensure data.shape matches\n",
    "    '''\n",
    "    subjects_slice = np.array([target == i for i in subjects]).any(axis=0) #or the list of lists\n",
    "\n",
    "    return subjects_slice\n",
    "\n",
    "def kFold(model, train_x, train_y, subjects, IDs):\n",
    "    k = 5\n",
    "    num_val_samples = len(subjects) // k\n",
    "    num_epochs = 500 \n",
    "    all_scores = [] \n",
    "    for i in range(k):\n",
    "           \n",
    "        print(f\"Processing fold #{i}\")\n",
    "        \n",
    "        if( i == (k-1)):\n",
    "            validation_indicies = get_user_slice(target=IDs, subjects=subjects[i * num_val_samples:len(subjects)])\n",
    "            print(\"Validation Set: Subjects \" + str(i * num_val_samples) + \":\" +  str(len(subjects)))\n",
    "\n",
    "        else:\n",
    "            validation_indicies = get_user_slice(target=IDs, subjects=subjects[i * num_val_samples: (i + 1) * num_val_samples])\n",
    "            print(\"Validation Set: Subjects \" + str(i * num_val_samples) + \":\" +  str((i + 1) * num_val_samples))\n",
    "            \n",
    "        train_indicies = np.invert(validation_indicies)\n",
    "                                   \n",
    "        val_data = train_x[validation_indicies]\n",
    "        val_targets = train_y[validation_indicies]\n",
    "        \n",
    "        partial_train_data = train_x[train_indicies]\n",
    "        partial_train_targets = train_y[train_indicies]\n",
    "                \n",
    "        model.fit(partial_train_data, partial_train_targets,                    \n",
    "                  epochs=num_epochs, batch_size=1, verbose=0)\n",
    "        loss, val_acc = model.evaluate(val_data, val_targets, verbose=0)     \n",
    "        all_scores.append(val_acc)\n",
    "    #print(\"Fold Accuracies: \", all_scores)\n",
    "    #print(\"Average Accuracy: \", np.mean(all_scores))\n",
    "    \n",
    "    return np.mean(all_scores)\n",
    "\n",
    "# participant class that holds datasets for all participants\n",
    "class Participant:\n",
    "\n",
    "    def __init__(self, baseline_fileName, coldpressor_fileName,  participantNumber):\n",
    "\n",
    "        self.cutoff = 99999999\n",
    "        \n",
    "        ########################################### Load Baseline Data ############################################\n",
    "        \n",
    "        baseline_input_data = pandas.read_csv(baseline_fileName)\n",
    "        # print(baseline_input_data.head())\n",
    "        self.unique_baseline_values = baseline_input_data[\"ibi\"].unique()\n",
    "        baseline_input_data.describe()\n",
    "        baseline_data = baseline_input_data.values\n",
    "\n",
    "        # store all values into Participant variables\n",
    "        self.baseline_participantNumber = participantNumber\n",
    "        self.baseline_id = baseline_data[:,[0]] # store all rows, id column\n",
    "        self.baseline_user_id = baseline_data[:,[1]] # store all rows, user_id column\n",
    "        self.baseline_unix_in_ms = baseline_data[baseline_data[:, 2].argsort()] # store all rows, unix_in_ms column sorted\n",
    "        self.baseline_ibi = baseline_data[:,[3]] # store all rows, ibi column\n",
    "        self.baseline_raw_data = baseline_data\n",
    "        \n",
    "        # get sorted unix time and ibi to store into plotting variable\n",
    "        \n",
    "        baseline_data = baseline_data[:,[2,3]] # Get all rows, time and ibi columns\n",
    "        baseline_data = baseline_data[baseline_data[:, 0].argsort()]  # Sort by time to ensure time is chronological\n",
    "        \n",
    "        # print(baseline_data)\n",
    "        \n",
    "        self.baseline_unix_vs_ibi_for_plotting = baseline_data[:,[0,1]] #variable to be used for plotting\n",
    "       # x axis timing\n",
    "        baseline_plot_width_seconds = baseline_data[0,0] - baseline_data[0,1]\n",
    "        baseline_min_time = baseline_data[1,0]\n",
    "        baseline_max_time = baseline_min_time + baseline_plot_width_seconds\n",
    "\n",
    "        # define indices and x values\n",
    "        self.baseline_indices = get_indices(baseline_data[:,0], baseline_min_time, baseline_max_time)\n",
    "        self.baseline_x_vals = [datetime.utcfromtimestamp(t/1000) for t in baseline_data[self.baseline_indices,0]] #convert to datetime objects\n",
    "\n",
    "        ########################################### Load Cold Pressor Data ############################################\n",
    "        \n",
    "        coldpressor_input_data = pandas.read_csv(coldpressor_fileName)\n",
    "        self.unique_coldpressor_values = coldpressor_input_data[\"ibi\"].unique()\n",
    "        coldpressor_input_data.describe()\n",
    "        coldpressor_data = coldpressor_input_data.values\n",
    "        # print(input_data)\n",
    "\n",
    "        # store all values into Participant variables\n",
    "        self.coldpressor_participantNumber = participantNumber\n",
    "        self.coldpressor_id = coldpressor_data[:,[0]] # store all rows, id column\n",
    "        self.coldpressor_user_id = coldpressor_data[:,[1]] # store all rows, user_id column\n",
    "        self.coldpressor_unix_in_ms = coldpressor_data[coldpressor_data[:, 2].argsort()] # store all rows, unix_in_ms column sorted\n",
    "        self.coldpressor_ibi = coldpressor_data[:,[3]] # store all rows, ibi column\n",
    "        self.coldpressor_raw_data = coldpressor_data\n",
    "        \n",
    "        # get sorted unix time and ibi to store into plotting variable\n",
    "        \n",
    "        coldpressor_data = coldpressor_data[:,[2,3]] # Get all rows, time and ibi columns\n",
    "        coldpressor_data = coldpressor_data[coldpressor_data[:, 0].argsort()]  # Sort by time to ensure time is chronological\n",
    "        \n",
    "        \n",
    "        self.coldpressor_unix_vs_ibi_for_plotting = coldpressor_data[:,[0,1]] #variable to be used for plotting\n",
    "        \n",
    "       # x axis timing\n",
    "        coldpressor_plot_width_seconds = coldpressor_data[0,0] - coldpressor_data[0,1]\n",
    "        coldpressor_min_time = coldpressor_data[1,0]\n",
    "        coldpressor_max_time = coldpressor_min_time + coldpressor_plot_width_seconds\n",
    "\n",
    "        # define indices and x values\n",
    "        self.coldpressor_indices = get_indices(coldpressor_data[:,0], coldpressor_min_time, coldpressor_max_time)\n",
    "        self.coldpressor_x_vals = [datetime.utcfromtimestamp(t/1000) for t in coldpressor_data[self.coldpressor_indices,0]] #convert to datetime objects\n",
    "        \n",
    "        self.baseline_data_length = len(self.baseline_unix_vs_ibi_for_plotting[:,1])\n",
    "        self.coldpressor_data_length = len(self.coldpressor_unix_vs_ibi_for_plotting[:,1])\n",
    "        \n",
    "    # getter and setter methods\n",
    "    \n",
    "    #plot participant baseline data\n",
    "    def plot_baseline(self):\n",
    "    \n",
    "        # decorate\n",
    "        fig = plt.figure(self.baseline_participantNumber)\n",
    "        ax = plt.axes()\n",
    "        plt.grid()\n",
    "        plt.title(\"Participant \" + str(self.baseline_participantNumber) + \" Baseline Test\")\n",
    "        plt.xlabel('Time (Unix)', fontsize=15)\n",
    "        plt.ylabel('IBI Measurement (ms)', fontsize=15)\n",
    "        plt.rcParams['figure.figsize'] = plot_size\n",
    "\n",
    "        indices = self.baseline_indices\n",
    "\n",
    "        # plot\n",
    "        plt.plot(self.baseline_x_vals,self.baseline_unix_vs_ibi_for_plotting[indices,1], 'o', linestyle = '--')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    #plot participant data\n",
    "    def plot_coldpressor(self):\n",
    "    \n",
    "        # decorate\n",
    "        fig = plt.figure(self.coldpressor_participantNumber)\n",
    "        ax = plt.axes()\n",
    "        plt.grid()\n",
    "        plt.title(\"Participant \" + str(self.coldpressor_participantNumber) + \" Cold Pressor Test\")\n",
    "        plt.xlabel('Time (Unix)', fontsize=15)\n",
    "        plt.ylabel('IBI Measurement (ms)', fontsize=15)\n",
    "        plt.rcParams['figure.figsize'] = plot_size\n",
    "\n",
    "        indices = self.coldpressor_indices\n",
    "\n",
    "        # plot\n",
    "        plt.plot(self.coldpressor_x_vals,self.coldpressor_unix_vs_ibi_for_plotting[indices,1], 'o', linestyle = '--', color = 'ORANGE')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_both(self):\n",
    "        \n",
    "        # decorate\n",
    "        fig = plt.figure(self.coldpressor_participantNumber)\n",
    "        ax = plt.axes()\n",
    "        plt.grid()\n",
    "        plt.title(\"Participant \" + str(self.coldpressor_participantNumber) + \" Baseline And Cold Pressor Test\")\n",
    "        plt.xlabel('Time (Unix)', fontsize=15)\n",
    "        plt.ylabel('IBI Measurement (ms)', fontsize=15)\n",
    "        plt.rcParams['figure.figsize'] = plot_size\n",
    "        \n",
    "        baseline_indices = self.baseline_indices\n",
    "        coldpressor_indices = self.coldpressor_indices\n",
    "\n",
    "        # plot\n",
    "        line1, = plt.plot(self.baseline_x_vals,self.baseline_unix_vs_ibi_for_plotting[baseline_indices,1], 'o', linestyle = '--', label = 'Baseline Test')\n",
    "        line2, = plt.plot(self.coldpressor_x_vals,self.coldpressor_unix_vs_ibi_for_plotting[coldpressor_indices,1], 'o', linestyle = '--', color = 'ORANGE', label = 'Cold Pressor Test')\n",
    "        \n",
    "        # legend\n",
    "        plt.legend(handles=[line1, line2], loc='upper left', fontsize = 'x-large')\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def normalizeBaselineData(self):\n",
    "        \n",
    "        baseline_normalizedData = self.baseline_unix_vs_ibi_for_plotting #initialize baseline_normalizedData variable\n",
    "        \n",
    "        maxBaselineValue = max(self.baseline_unix_vs_ibi_for_plotting[:,1]) #set maxValue to highest value of ibi signals\n",
    "        minBaselineValue = min(self.baseline_unix_vs_ibi_for_plotting[:,1]) #set minValue to lowest value of ibi signals\n",
    "        maxColdPressorValue = max(self.coldpressor_unix_vs_ibi_for_plotting[:,1]) #set maxValue to highest value of ibi signals\n",
    "        minColdPressorValue = min(self.coldpressor_unix_vs_ibi_for_plotting[:,1]) #set minValue to lowest value of ibi signals\n",
    "        maxValue = max(maxBaselineValue, maxColdPressorValue)\n",
    "        minValue = min(minBaselineValue, minColdPressorValue)\n",
    "        \n",
    "        \n",
    "        numRows = len(self.baseline_unix_vs_ibi_for_plotting) #get number of rows\n",
    "        \n",
    "        for i in range(numRows):\n",
    "            baseline_normalizedData[i,1] = (self.baseline_unix_vs_ibi_for_plotting[i,1] - minValue)/(maxValue-minValue)\n",
    "        \n",
    "        self.baseline_unix_vs_ibi_for_plotting = baseline_normalizedData\n",
    "        return baseline_normalizedData\n",
    "        \n",
    "    def normalizeColdpressorData(self):\n",
    "        \n",
    "        coldpressor_normalizedData = self.coldpressor_unix_vs_ibi_for_plotting #initialize coldpressor_normalizedData variable\n",
    "        \n",
    "        maxBaselineValue = max(self.baseline_unix_vs_ibi_for_plotting[:,1]) #set maxValue to highest value of ibi signals\n",
    "        minBaselineValue = min(self.baseline_unix_vs_ibi_for_plotting[:,1]) #set minValue to lowest value of ibi signals\n",
    "        maxColdPressorValue = max(self.coldpressor_unix_vs_ibi_for_plotting[:,1]) #set maxValue to highest value of ibi signals\n",
    "        minColdPressorValue = min(self.coldpressor_unix_vs_ibi_for_plotting[:,1]) #set minValue to lowest value of ibi signals\n",
    "        maxValue = max(maxBaselineValue, maxColdPressorValue)\n",
    "        minValue = min(minBaselineValue, minColdPressorValue)\n",
    "        \n",
    "        numRows = len(self.coldpressor_unix_vs_ibi_for_plotting) #get number of rows\n",
    "        \n",
    "        for i in range(numRows):\n",
    "            coldpressor_normalizedData[i,1] = (self.coldpressor_unix_vs_ibi_for_plotting[i,1] - minValue)/(maxValue-minValue)\n",
    "        \n",
    "        self.coldpressor_unix_vs_ibi_for_plotting = coldpressor_normalizedData\n",
    "        return coldpressor_normalizedData\n",
    "    \n",
    "#     def createTrainTestColdPressorDataset(self):\n",
    "        \n",
    "#         trainColdPressorDataset, testColdPressorDataset = train_test_split(self.coldpressor_unix_vs_ibi_for_plotting[:,:], shuffle=False, train_size = .75)\n",
    "        \n",
    "#         self.trainColdPressorDataset = trainColdPressorDataset\n",
    "#         self.testColdPressorDataset = testColdPressorDataset\n",
    "    \n",
    "#     def createTrainTestBaselineDataset(self):\n",
    "        \n",
    "#         trainBaselineDataset, testBaselineDataset = train_test_split(self.baseline_unix_vs_ibi_for_plotting[:,:], shuffle=False, train_size = .75)\n",
    "        \n",
    "#         self.trainBaselineDataset = trainBaselineDataset\n",
    "#         self.testBaselineDataset = testBaselineDataset\n",
    "    \n",
    "#     def splitDataSampleLevel(self):\n",
    "        \n",
    "#         if(self.baseline_data_length > self.coldpressor_data_length):\n",
    "#             length = self.coldpressor_data_length\n",
    "#             self.baseline_unix_vs_ibi_for_plotting = np.delete(self.baseline_unix_vs_ibi_for_plotting, slice(length,self.baseline_data_length), axis=0)\n",
    "#             self.baseline_data_length = len(self.baseline_unix_vs_ibi_for_plotting[:,1])\n",
    "#             return self.baseline_unix_vs_ibi_for_plotting\n",
    "#         else:\n",
    "#             length = self.baseline_data_length\n",
    "#             self.coldpressor_unix_vs_ibi_for_plotting = np.delete(self.coldpressor_unix_vs_ibi_for_plotting, slice(length,self.coldpressor_data_length), axis=0)\n",
    "#             self.coldpressor_data_length = len(self.coldpressor_unix_vs_ibi_for_plotting[:,1])\n",
    "#             return self.coldpressor_unix_vs_ibi_for_plotting\n",
    "        \n",
    "    def splitDataKeepLeft(self, cutoff):\n",
    "    \n",
    "            self.baseline_unix_vs_ibi_for_plotting = np.delete(self.baseline_unix_vs_ibi_for_plotting, slice(cutoff,self.baseline_data_length), axis=0)\n",
    "            self.coldpressor_unix_vs_ibi_for_plotting = np.delete(self.coldpressor_unix_vs_ibi_for_plotting, slice(cutoff,self.coldpressor_data_length), axis=0)\n",
    "\n",
    "            self.baseline_data_length = len(self.baseline_unix_vs_ibi_for_plotting[:,1])\n",
    "            self.coldpressor_data_length = len(self.coldpressor_unix_vs_ibi_for_plotting[:,1])\n",
    "            \n",
    "        \n",
    "    def splitDataKeepRight(self, cutoff):\n",
    "    \n",
    "            self.baseline_unix_vs_ibi_for_plotting = np.delete(self.baseline_unix_vs_ibi_for_plotting, slice(0,self.baseline_data_length-cutoff), axis=0)\n",
    "            self.coldpressor_unix_vs_ibi_for_plotting = np.delete(self.coldpressor_unix_vs_ibi_for_plotting, slice(0,self.coldpressor_data_length-cutoff), axis=0)\n",
    "\n",
    "            self.baseline_data_length = len(self.baseline_unix_vs_ibi_for_plotting[:,1])\n",
    "            self.coldpressor_data_length = len(self.coldpressor_unix_vs_ibi_for_plotting[:,1])\n",
    "\n",
    "        \n",
    "    def getfeatures(self):            \n",
    "        \n",
    "        # Using Skicit-learn to split data into training and testing sets\n",
    "        # Split the data into training and testing sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(self.features, self.labels, test_size = 0.25, random_state = 42)\n",
    "        \n",
    "        # Instantiate model with 1000 decision trees\n",
    "        rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "        \n",
    "        # Train the model on training data\n",
    "        rf.fit(train_features, train_labels);\n",
    "        \n",
    "    # id getter method\n",
    "    def get_baseline_unix_vs_ibi_for_plotting(self):\n",
    "        return self.baseline_unix_vs_ibi_for_plotting\n",
    "\n",
    "    # user_id getter method\n",
    "    def getUser_ID(self):\n",
    "        return self.user_id\n",
    "\n",
    "    # unix_in_ms getter method\n",
    "    def getUnix_in_ms(self):\n",
    "        return self.unix_in_ms\n",
    "\n",
    "    # ibi getter method\n",
    "    def getIbi(self):\n",
    "        return self.ibi\n",
    "    \n",
    "    # ibi getter method\n",
    "    def getData(self):\n",
    "        return self.data\n",
    "\n",
    "    # id setter method\n",
    "    def setID(self, id):\n",
    "        self.id = id\n",
    "\n",
    "    # user_id setter method\n",
    "    def setUser_ID(self, user_id):\n",
    "        self.user_id = user_id\n",
    "\n",
    "    # unix_in_ms setter method\n",
    "    def setUnix_in_ms(self, unix_in_ms):\n",
    "        self.unix_in_ms = unix_in_ms\n",
    "\n",
    "    # ibi setter method\n",
    "    def setIbi(self, ibi):\n",
    "        self.ibi = ibi\n",
    "        \n",
    "    def getBaselineSize(self):\n",
    "        return self.baseline_data_length\n",
    "    \n",
    "    def getColdpressorSize(self):\n",
    "        return self.coldpressor_data_length\n",
    "    \n",
    "p = 0\n",
    "rf_accuracies = []\n",
    "rf_kfold_accuracies = []\n",
    "dense_accuracies = []\n",
    "dense_kfold_accuracies = []\n",
    "regression_accuracies = []\n",
    "\n",
    "while(p<10):\n",
    "    #main\n",
    "    \n",
    "    print(\"Running Trial #\" + str(p))\n",
    "\n",
    "    #init participants\n",
    "    participants = [Participant for i in range(61)]\n",
    "\n",
    "    #store all participant data\n",
    "    n = 1\n",
    "    while (n<61):\n",
    "        if(n != 2 and (n < 17 or n > 31) and n != 5): #No participants 2,17-31. Participant 5 has missing values\n",
    "            participants[n] = Participant(\"C:\\\\Users\\\\C23Price.Johnson\\\\Desktop\\\\Capstone\\\\Capstone Project\\\\Pain-Identification\\\\src\\\\raw_data\\\\raw_data_garmin_unique_values\\\\participant_\" + str(n) + \"_BL.csv\",\n",
    "                                          \"C:\\\\Users\\\\C23Price.Johnson\\\\Desktop\\\\Capstone\\\\Capstone Project\\\\Pain-Identification\\\\src\\\\raw_data\\\\raw_data_garmin_unique_values\\\\participant_\" + str(n) + \"_CPT.csv\",n)\n",
    "        n = n + 1\n",
    "\n",
    "    # Normalize Data\n",
    "    n = 1\n",
    "    while (n<61):\n",
    "        if(n != 2 and (n < 17 or n > 31) and n != 5): #No participants 2,17-31. Participant 5 has missing values\n",
    "            participants[n].normalizeBaselineData()\n",
    "            participants[n].normalizeColdpressorData()\n",
    "        n = n + 1\n",
    "\n",
    "    #initiate variables\n",
    "    baselineTotal = 0\n",
    "    coldPressorTotal = 0\n",
    "    numParticipants = 0\n",
    "    smallestBaselineSize = 99999999\n",
    "    smallestColdpressorSize = 99999999\n",
    "    cutoff = 200\n",
    "    numberOfUsableParticipants = 0\n",
    "\n",
    "    #get average number of baseline and coldpressor datapoints\n",
    "    n = 1\n",
    "    while (n<61):\n",
    "        if(n != 2 and (n < 17 or n > 31) and n != 5): #No participants 2,17-31. Participant 5 has missing values\n",
    "            baselineTotal = baselineTotal + participants[n].getBaselineSize()\n",
    "            coldPressorTotal = coldPressorTotal + participants[n].getColdpressorSize()\n",
    "            numParticipants = numParticipants + 1\n",
    "            smallestBaselineSize = min(smallestBaselineSize, participants[n].baseline_data_length)\n",
    "            smallestColdpressorSize = min(smallestColdpressorSize, participants[n].coldpressor_data_length)\n",
    "        n = n + 1\n",
    "\n",
    "    averageNumberOfBaselineDatapoints = round((baselineTotal / numParticipants), 0)\n",
    "    averageNumberOfColdpressorDatapoints = round((coldPressorTotal / numParticipants), 0)\n",
    "\n",
    "    IDs = []\n",
    "    lengths = []\n",
    "    n=1\n",
    "    while (n<61):\n",
    "        if(n != 2 and (n < 17 or n > 31) and n != 5): #No participants 2,17-31. Participant 5 has missing values\n",
    "            IDs.append(n)\n",
    "            lengths.append(participants[n].coldpressor_data_length)\n",
    "        n = n + 1\n",
    "\n",
    "    # Get all usable participant data that where usable participant's dataset size if greater than the cutoff\n",
    "    usableParticipants = [Participant for i in range(1)]\n",
    "    n = 1\n",
    "    while (n<61 and participants[n] != None):\n",
    "        if(n != 2 and (n < 17 or n > 31) and n != 5): #No participants 2,17-31. Participant 5 has missing values\n",
    "            if(((participants[n].getBaselineSize() >= cutoff) and (participants[n].getColdpressorSize() >= cutoff)) or (not participants[n])):\n",
    "                usableParticipants.append(participants[n])\n",
    "                numberOfUsableParticipants = numberOfUsableParticipants + 1\n",
    "        n = n + 1\n",
    "\n",
    "    #split data into same size\n",
    "    n = 1\n",
    "    while (n<=numberOfUsableParticipants):\n",
    "        usableParticipants[n].splitDataKeepRight(cutoff)\n",
    "        n = n + 1\n",
    "\n",
    "    # create samples and labels array based on usable participants\n",
    "    samples = [usableParticipants[1].baseline_unix_vs_ibi_for_plotting for i in range(1)]\n",
    "    samples[0] = usableParticipants[1].baseline_unix_vs_ibi_for_plotting[:,1]\n",
    "    labels = [0 for x in range(0, numberOfUsableParticipants)]\n",
    "    ID = []\n",
    "    ID.append(1)\n",
    "    n = 2\n",
    "    while (n<=numberOfUsableParticipants):\n",
    "        samples.append(usableParticipants[n].baseline_unix_vs_ibi_for_plotting[:,1])\n",
    "        ID.append(n)\n",
    "        n = n + 1 \n",
    "    n = 1\n",
    "    while (n<=numberOfUsableParticipants):\n",
    "        samples.append(usableParticipants[n].coldpressor_unix_vs_ibi_for_plotting[:,1])\n",
    "        labels.append(1)\n",
    "        ID.append(n)\n",
    "        n = n + 1\n",
    "\n",
    "    # Saving feature names for later use\n",
    "    feature_names = [\"IBI\", \"Pain\"]\n",
    "\n",
    "    #parallel arrays, same indicies\n",
    "    samples = np.array(samples).astype(np.float32)\n",
    "    labels = np.array(labels)\n",
    "    ID = np.array(ID)\n",
    "\n",
    "    subjects = ID[0:numberOfUsableParticipants] # subjects to shuffle on\n",
    "\n",
    "    cut = .2\n",
    "\n",
    "    np.random.shuffle(subjects)\n",
    "    split_count = int(cut * numberOfUsableParticipants) #porportion of train and test splits\n",
    "\n",
    "    #indicies from 0 to split_count\n",
    "    validation_indicies = get_user_slice(target=ID, subjects=subjects[0:split_count]) \n",
    "\n",
    "    #indicies from split_count to 2xsplit_count\n",
    "    test_indicies = get_user_slice(target=ID, subjects=subjects[split_count:2 * split_count]) \n",
    "    train_indicies = np.invert(np.array([test_indicies,validation_indicies]).any(axis=0))\n",
    "\n",
    "    train_x = samples[train_indicies]\n",
    "    train_y = labels[train_indicies]\n",
    "    val_x   = samples[validation_indicies]\n",
    "    val_y   = labels[validation_indicies]\n",
    "    test_x  = samples[test_indicies]\n",
    "    test_y  = labels[test_indicies]\n",
    "\n",
    "    kfold_subjects = subjects\n",
    "    k_fold_IDs_indicies = np.invert(np.array(test_indicies))\n",
    "    k_fold_IDs = ID[k_fold_IDs_indicies]\n",
    "    temp_array = subjects[split_count:2 * split_count]\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < (kfold_subjects.size):\n",
    "        j = 0\n",
    "        while j < (temp_array.size):\n",
    "            if(kfold_subjects[i] == temp_array[j]):\n",
    "                kfold_subjects = np.delete(kfold_subjects,i)\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "\n",
    "    # Instantiate model with 1000 decision trees, use classifier for binary classification \n",
    "    rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_x, train_y);\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = rf.predict(test_x)\n",
    "\n",
    "    # Find which values were correctly predicted\n",
    "    correctPredictions =  [0 for x in range(0, len(test_y))]\n",
    "    n = 0\n",
    "    numCorrect = 0\n",
    "    while (n<len(test_y)):\n",
    "        correctPredictions[n] = int (not (test_y[n] ^ predictions[n]))\n",
    "        if(correctPredictions[n] == 1):\n",
    "            numCorrect = numCorrect + 1\n",
    "        n = n + 1\n",
    "\n",
    "    #K fold\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(ensemble.RandomForestClassifier(random_state= 42), train_x, train_y, cv= kf, scoring=\"accuracy\")\n",
    "\n",
    "    #print(\"Accuracy: \" + str( round( ((numCorrect/len(test_y)) * 100), 2)) + \"%\")\n",
    "    #print(f'K Fold Average score: {\"{:.2f}\".format(score.mean()*100)}' + '%')\n",
    "    \n",
    "    rf_accuracies.append(round( ((numCorrect/len(test_y)) * 100), 2))\n",
    "    rf_kfold_accuracies.append(round(score.mean()*100, 2))\n",
    "    \n",
    "    k_train_x = np.vstack((train_x, val_x))\n",
    "    k_train_y = np.hstack((train_y, val_y))\n",
    "    \n",
    "    # Practice with Keras Model\n",
    "    # Activation and Loss Function\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(1, input_shape=(cutoff,), activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l1'))\n",
    "    #model.add(Dense(128, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l2'))\n",
    "    # model.add(Dense(64, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l2'))\n",
    "    #model.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l2'))\n",
    "    #model.add(Dense(16, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l2'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l1'))\n",
    "    #model.add(Dense(4, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l1'))\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    #model.add(Dense(2, activation='relu', kernel_initializer = 'he_normal', kernel_regularizer = 'l2'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer = 'he_normal'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate = .00025), metrics=['acc'])\n",
    "    #model.summary()\n",
    "\n",
    "    # fit the keras model on the dataset\n",
    "    history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=800, batch_size=1, verbose=0)\n",
    "\n",
    "    # Print accuracy of the model, anything greater than 50% would be great\n",
    "    _, accuracy = model.evaluate(test_x, test_y)\n",
    "    print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "    #K fold\n",
    "\n",
    "    k_train_x = np.vstack((train_x, val_x))\n",
    "    k_train_y = np.hstack((train_y, val_y))\n",
    "\n",
    "    #kFold(model, k_train_x, k_train_y, kfold_subjects, k_fold_IDs)\n",
    "    dense_accuracies.append(round( (accuracy*100), 2))\n",
    "    dense_kfold_accuracies.append(round(kFold(model, k_train_x, k_train_y, kfold_subjects, k_fold_IDs)*100, 2))\n",
    "    \n",
    "    # instantiate the model (using the default parameters)\n",
    "    model = LogisticRegression(random_state=0)\n",
    "\n",
    "    # fit the model with data\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    pred_y = model.predict(test_x)\n",
    "\n",
    "    #print(\"Before Grid Accuracy: \" + str(model.score(test_x , test_y) * 100) + \"%\")\n",
    "\n",
    "    # messing around with grid to see if it improves accuracy\n",
    "\n",
    "    # instantiate parameters\n",
    "    params = {\n",
    "    \"max_iter\": [20, 50, 100, 200],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "    \"class_weight\": [\"balanced\"]\n",
    "    }\n",
    "\n",
    "    # grid fit\n",
    "    logModel_grid = GridSearchCV(estimator=LogisticRegression(random_state=0),scoring =\"accuracy\", param_grid=params, verbose=1, cv=10, n_jobs=2)\n",
    "    logModel_grid.fit(train_x, train_y)\n",
    "\n",
    "    # grid predict\n",
    "    pred_y_grid = logModel_grid.predict(test_x)\n",
    "\n",
    "    #print(\"After Grid Accuracy: \" + str(accuracy_score(test_y, pred_y_grid) * 100) + \"%\")\n",
    "\n",
    "    regression_accuracies.append(round(accuracy_score(test_y, pred_y_grid) * 100))\n",
    "\n",
    "    p = p + 1\n",
    "print(\"Random Forrests Accuracies: \")\n",
    "print(rf_accuracies)\n",
    "print(\"Random Forrests KFold Accuracies: \")\n",
    "print(rf_kfold_accuracies)\n",
    "print(\"Dense Accuracies: \")\n",
    "print(dense_accuracies)\n",
    "print(\"Dense KFold Accuracies: \")\n",
    "print(dense_kfold_accuracies)\n",
    "print(\"Regression Accuracies: \")\n",
    "print(regression_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde827c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
